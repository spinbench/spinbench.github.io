<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="How Well Do Large Language Models Reason Strategically and Socially?">
  <meta name="keywords" content="SPINBench, LLM Agents, Strategic Reasoning, LLMs, Social Intelligence">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-KXWQ1327FT"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-KXWQ1327FT');
  </script>
  
  <title>SPIN-Bench: How Well Do Large Language Models Reason Strategically and Socially?</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/tool_cards.css">
  <link rel="stylesheet" href="./static/css/top_button.css">
  <link rel="stylesheet" href="./static/css/navbar.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>

  <!-- Add navbar.js script -->
  <script src="./static/js/navbar.js"></script>

  <!-- Add MathJax -->
  <script src="./static/js/mathjax.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Add visualization.js script -->
  <script src="./static/js/visualization.js"></script>

</head>


<body>
  <!-- Add Table of Contents -->
  <div class="toc-wrapper">
    <h4 style="margin-bottom: 10px;">
      <a href="#">Table of Contents</a>
    </h4>
    <ul class="toc-list">
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#framework">Framework</a></li>
      <li><a href="#game-trajectory">Game Trajectory Visualization</a></li>
      <li><a href="#solver-trajectories">LLM vs Solver Game Trajectories</a></li>
      <li><a href="#visualization">Game Settings and Evaluation Metrics</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#Share">Share</a></li>
      <li><a href="#BibTeX">BibTeX</a></li>
    </ul>
  </div>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
              <span style="vertical-align: middle">
                <span style="color: var(--color-octo-red);">SPIN</span>-<span style="color: var(--color-tools-blue);">Bench</span>
              </span>
            </h1>
            <h2 class="subtitle is-3 publication-subtitle">
              How Well Do LLMs Plan Strategically and Reason Socially?
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ece.princeton.edu/people/jianzhu-yao" target="_blank">Jianzhu Yao</a>*
                <a href="mailto:jy0246@princeton.edu"><svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a>,</span>
              <span class="author-block">
                <a href="https://www.kevin-ai.com/" target="_blank">Kevin Wang</a>*
                <a href="mailto:kevinwang.1839@utexas.edu"><svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a>,</span>
              <span class="author-block">
                Ryan Hsieh,</span>
              <span class="author-block">
                Haisu Zhou,</span>
              <span class="author-block">
                Tianqing Zou,</span>
              <span class="author-block">
                Zerui Cheng,</span>
              <span class="author-block">
                <a href="https://vita-group.github.io/" target="_blank">Zhangyang Wang</a>
                <a href="mailto:atlaswang@utexas.edu"><svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a>,</span>
              <span class="author-block">
                <a href="https://ece.princeton.edu/people/pramod-viswanath" target="_blank">Pramod Viswanath</a>
                <a href="mailto:pramodv@princeton.edu"><svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a></span>
            </div>
          
            <div class="is-size-5 publication-authors">
              <span class="author-block">Princeton University, The University of Texas at Austin</span><br>
              <span class="paper-block"><b>* Equal Contribution</b></span>
            </div>
            <!-- paper link -->
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2503.12349.pdf"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- arxiv link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.12349" class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <img src="assets/models/main_table.png" width="100%">
        <p style="text-align: left;">
          Overview of the Strategic Planning, Interaction, and Negotiation (SPIN-Bench) framework, highlighting its two core components: (1) the Game Agent, which
          encompasses the LLMs and their adaptive prompting, and (2) the Environment and Evaluation subsystem,
          which manage game logic, track interactions, and quantify performance
        </p>
      </div>
    </div>
  </section>


  <!-- Introduction -->
  <section class="section" id="introduction">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              We introduce <strong>Strategic Planning, Interaction, and Negotiation (<span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span
                style="color: var(--color-tools-blue);"><b>Bench</b></span>)</strong>, a comprehensive framework
              for evaluating
              <em>long-horizon strategic planning</em> and <em>social intelligence</em> in Large Language Models (LLMs).
              Unlike prior work that confines itself to narrow planning or isolated single-agent tasks,
              <strong><span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span
                style="color: var(--color-tools-blue);"><b>Bench</b></span></strong> combines formal PDDL challenges, competitive board games, cooperative card games,
              and multi-agent negotiation scenarios within a single evaluation.
            </p>
            
            <p>
              By systematically varying action spaces, state complexity, and the number of interacting agents,
              <strong><span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span
                style="color: var(--color-tools-blue);"><b>Bench</b></span></strong> tests not only <em>methodical, step-wise</em> decision-making but also
              <em>conceptual inference</em> about hidden information and adversarial or cooperative strategies.
              Our experiments reveal that while contemporary LLMs handle <em>basic fact retrieval</em> and
              <em>short-range planning</em> reasonably well, they encounter significant performance bottlenecks
              in tasks requiring <em>deep multi-hop reasoning</em> over large state spaces and <em>socially adept</em>
              coordination under uncertainty.
            </p>
            
            <p>
              In particular, we find that strong models (e.g., <code>o1</code>) can still struggle with
              <em>extended-horizon planning</em> when multiple agents and hidden intentions are introduced,
              and that extensive social interaction can sometimes degrade chain-of-thought coherence.
              These insights highlight persistent gaps in <em>multi-agent negotiation</em>,
              <em>alliance formation</em>, and <em>perspective-taking</em>, underscoring where further
              advances in LLM architectures and training might be needed.
            </p>
            
            <p>
              By drawing on both human baselines and domain-specific solvers, our results shed light on the
              real-world potential and current shortcomings of LLMs for strategic, multi-agent settings.
              We envision <strong><span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span
                style="color: var(--color-tools-blue);"><b>Bench</b></span></strong> as a catalyst for future research on robust multi-agent planning,
              social reasoning, and human–AI teaming.
            </p>
          </div>

          <h3 class="title is-4">Task Taxonomy and Environments</h3>
          <div class="content has-text-justified">
            <p>
              The <span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span
              style="color: var(--color-tools-blue);"><b>Bench</b></span> framework integrates four distinct environment types:
            </p>
            <ol>
              <li><b>PDDL Tasks:</b> Classical planning problems across 21 domains (1,280 tasks) spanning factual retrieval, 
              spatial reasoning, and multi-step planning with increasing state spaces.</li>
              <li><b>Competitive Games:</b> Turn-based board games of escalating complexity (Tic-tac-toe, Connect Four, Chess) 
              that test adversarial reasoning from short-range tactics to deeper strategic thinking.</li>
              <li><b>Cooperative Games:</b> Featuring Hanabi, a card game where players see others' cards but not their own, 
              requiring trust-building, inference about hidden states, and coordinated actions.</li>
              <li><b>Strategic Games:</b> Incorporating Diplomacy, where negotiation, alliance formation, and strategic betrayal 
              are integral, testing both planning capabilities and social intelligence.</li>
            </ol>
            <p>
              This structured progression allows us to systematically pinpoint where LLM reasoning breaks down—whether in state tracking, 
              partial-order reasoning, chain-of-thought coherence, or dynamic social interaction. By combining these environments 
              within a unified evaluation framework, <span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span style="color: var(--color-tools-blue);"><b>Bench</b></span> provides unprecedented insight into how LLMs transition from 
              basic planning to complex multi-agent reasoning.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--/ Introduction -->

  <!-- Game Trajectory Visualization -->
  <section class="section" id="game-trajectory">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Game Trajectory Visualization</h2>
          <div class="content has-text-justified">
            <p>
              Our benchmark includes a diverse set of games and tasks that test strategic planning and social reasoning. Here are some examples of the game trajectories and tasks that we include in our benchmark:
            </p>
            <div class="columns is-multiline">
              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">🏁</span>
                        <a href="tools/pddl/tra.html" class="tool-name" target="_blank">PDDL</a>
                      </p>
                      <p class="tool-description">
                        Classical planning tasks testing core reasoning skills through factual retrieval, spatial reasoning, and multi-step planning across 21 domains with varying complexity.
                      </p>
                    </div>
                  </div>
                </div>
              </div>

              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">⭕</span>
                        <a href="tools/ttt/tra.html" class="tool-name" target="_blank">Tic Tac Toe</a>
                      </p>
                      <p class="tool-description">
                        A simple competitive game played on a 3×3 grid, evaluating LLMs' understanding of basic rules, turn-taking, and elementary strategic planning against solvers and other LLMs.
                      </p>
                    </div>
                  </div>
                </div>
              </div>

              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">🔴</span>
                        <a href="tools/connect_four/tra.html" class="tool-name" target="_blank">Connect Four</a>
                      </p>
                      <p class="tool-description">
                        An intermediate strategy game with a 6×7 vertical grid where players drop colored discs, requiring foresight to align four discs while blocking opponents' attempts.
                      </p>
                    </div>
                  </div>
                </div>
              </div>

              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">♟️</span>
                        <a href="tools/chess/tra.html" class="tool-name" target="_blank">Chess</a>
                      </p>
                      <p class="tool-description">
                        A complex strategic board game played on an 8×8 checkered board, testing advanced planning, deep calculation, pattern recognition, and sophisticated decision-making.
                      </p>
                    </div>
                  </div>
                </div>
              </div>

              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">🎆</span>
                        <a href="tools/hanabi/tra.html" class="tool-name" target="_blank">Hanabi</a>
                      </p>
                      <p class="tool-description">
                        A cooperative card game where players see everyone else's cards but not their own, testing coordination with partial information across teams of 2-5 LLM agents.
                      </p>
                    </div>
                  </div>
                </div>
              </div>

              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">🌍</span>
                        <a href="tools/diplomacy/tra.html" class="tool-name" target="_blank">Diplomacy</a>
                      </p>
                      <p class="tool-description">
                        A grand strategy game featuring seven European powers, testing negotiation skills, alliance formation, spatial reasoning, and complex strategic planning in a multi-agent environment.
                      </p>
                    </div>
                  </div>
                </div>
              </div>

            </div>

          </div>
        </div>
      </div>
  </section>
  <!--/ Game Trajectory Visualization -->

  <!-- Solver Trajectories -->
  <section class="section" id="solver-trajectories">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">LLM vs Solver Game Trajectories</h2>
          <div class="content has-text-justified">
            <p>
              To establish rigorous baselines, we evaluate LLMs against optimal or near-optimal solvers. These matchups reveal how models perform against mathematically perfect play, highlighting their strategic reasoning capabilities and limitations:
            </p>
            <div class="columns is-multiline">
              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">⭕</span>
                        <a href="tools/ttt/solver.html" class="tool-name" target="_blank">Tic Tac Toe vs Minimax</a>
                      </p>
                      <p class="tool-description">
                        LLMs compete against a perfect Minimax solver that never loses. This tests basic game understanding and ability to achieve draws through optimal play in a theoretically solved game.
                      </p>
                    </div>
                  </div>
                </div>
              </div>

              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">🔴</span>
                        <a href="tools/connect_four/solver.html" class="tool-name" target="_blank">Connect Four vs Solver</a>
                      </p>
                      <p class="tool-description">
                        LLMs play against the Connect Four solver implementation that can calculate optimal moves for any board position, testing deeper tactical awareness and multi-step planning capabilities.
                      </p>
                    </div>
                  </div>
                </div>
              </div>

              <div class="column is-one-third">
                <div class="card tool-card">
                  <div class="card-content">
                    <div class="content">
                      <p class="title is-4">
                        <span class="tool-emoji" style="font-size: 24px;">♟️</span>
                        <a href="tools/chess/solver.html" class="tool-name" target="_blank">Chess vs Stockfish</a>
                      </p>
                      <p class="tool-description">
                        LLMs face the Stockfish chess engine at different skill levels (0, 5, 10, 15, and 20). Even against reduced-strength engines, this reveals significant gaps in deep calculation.
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>
  <!--/ Solver Trajectories -->

  <!-- Game Settings and Evaluation Metrics -->
  <section class="section" id="visualization">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3" id="visualization">Game Settings and Evaluation Metrics</h2>
          
          <!-- Example Selection Buttons -->
          <div class="buttons is-centered" style="margin-bottom: 2rem;">
            <!-- First Row -->
            <div class="buttons has-addons is-centered" style="width: 100%; margin-bottom: 0.5rem;">
              <button class="button is-info is-outlined example-button" data-example="1">
                PDDL
              </button>
              <button class="button is-info is-outlined example-button" data-example="2">
                Tic Tac Toe
              </button>
              <button class="button is-info is-outlined example-button" data-example="3">
                Connect Four
              </button>
            </div>
            
            <!-- Second Row -->
            <div class="buttons has-addons is-centered" style="width: 100%;">
              <button class="button is-info is-outlined example-button" data-example="4">
                Chess
              </button>
              <button class="button is-info is-outlined example-button" data-example="5">
                Hanabi
              </button>
              <button class="button is-info is-outlined example-button" data-example="6">
                Diplomacy
              </button>
            </div>
          </div>

          <!-- Example Display Container -->
          <div id="example-container" style="width: 100%; height: 1000px; margin-bottom: 100px; border: none;">
            <!-- The iframe will be inserted here by JavaScript -->
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- Framework -->
  <section class="section" id="framework">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">The <span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span
              style="color: var(--color-tools-blue);"><b>Bench</b></span> Framework</h2>
          <div class="content has-text-justified">
            <p>
              Building on the motivations outlined in our introduction, <span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span
              style="color: var(--color-tools-blue);"><b>Bench</b></span>'s architecture is organized around three progressively complex
              problem settings for automated action selection: <b>Classical Planning</b> (single-agent, deterministic), 
              <b>Multi-Agent Games</b> (cooperative or competitive), and <b>Strategic Games</b> (mixed cooperation, competition, and negotiation).
              Each setting introduces additional layers of complexity, requiring increasingly sophisticated reasoning capabilities.
            </p>
            <p>
              The framework consists of two core components: (1) the <b>Game Agent</b>, which encompasses the LLMs and their adaptive prompting,
              and (2) the <b>Environment and Evaluation</b> subsystem, which manages game logic, tracks interactions, and quantifies performance.
              Our flexible interface feeds models the current state description, relevant history, and legal actions, enabling standardized
              evaluation across diverse scenarios while maintaining game-specific requirements.
            </p>
            <p>
              For evaluation, we employ multiple metrics tailored to each environment type. Our <b>rule-based metrics</b> include accuracy 
              and N-Step Look Ahead for planning tasks, move quality comparison against solvers for competitive games, and final scores 
              for cooperative scenarios. We maintain <b>leaderboard-based comparisons</b> with internal Elo ratings to gauge relative 
              performance across models and against human baselines. For negotiation-heavy settings, we utilize six fine-grained, 
              <b>LLM-assisted negotiation metrics</b> that analyze message-strategy alignment, proposal acceptance, deal equity, 
              conflict tendencies, perspective-taking, and conditional negotiation abilities.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--/ Framework -->

  <!-- Results -->
  <section class="section" id="results">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2>
          <div class="content has-text-justified">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/main-table.png" width="90%">
                </div>
              </div>
              
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/spatial-tracking-accuracy.png" width="90%">
                  <p>To investigate whether LLMs' planning deficits stem from weaker spatial understanding, we designed tasks requiring each model to track positions across sequences of relative movements. This figure plots the accuracy of each model against the length of the movement trajectory. Notably, <code>o1-mini</code> and <code>GPT-4o</code> exhibit declining performance as the number of steps increases, whereas <code>o1</code> sustains <strong>perfect</strong> accuracy (100%) up to 29 steps. </p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/factual-tracking-accuracy.png" width="90%">
                  <p>Here, we investigate whether LLMs can reliably retrieve key facts from a planning trajectory. This figure illustrates how retrieval accuracy varies with trajectory length. Notably, <code>o1</code> performs most consistently, confirming that it "reads" multi-step expansions more accurately than either <code>GPT-4o</code> or <code>o1-mini</code></p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/diplomacy-factual-heatmap.png" width="90%">
                  <p>In Diplomacy, we design and categorize several factual queries into <strong>one-hop</strong> vs. <strong>multi-hop</strong> to further check models' factual retrieval in a highly strategic environment. The figure shows that nearly all LLMs do well on basic location or adjacency checks but degrade by a large margin on "Attackable" and "Attack Analysis," which demand deeper, multi-hop inference. Again, <code>o1</code> and <code>o1-preview</code> lead, but still exhibit significant drops compared to simpler tasks.</p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/action-state-space.png" width="90%">
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/game-leaderboard.png" width="90%">
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/solver-win-rate.png" width="90%">
                </div>
                <p>The Table shows that solvers always win or draw the game. Tic-tac-toe reveals that advanced LLMs (e.g., <code>o1</code>, <code>GPT-4-turbo</code>, <code>Claude 3.5 Sonnet</code>) can achieve draws <strong>some</strong> of the time, but typically still lose or draw to the perfect solver. In Connect Four and Chess, the gap widens: our solver and Stockfish-level engines maintain a 100% win rate across all tested LLMs.</p>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/top-move-percentage.png" width="90%">
                  <p>The Top Move distribution shows that while LLMs sometimes pick optimal moves in Connect Four, their accuracy drops drastically in Chess, underscoring how deeper tactics and branching expansions are beyond current LLMs' capacity.</p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/multi-agent-diplomacy-result.png" width="90%">
                  <p>Diplomacy also allows variable numbers of participating powers. Detailed results of more multi-agent settings are shown here. As the agent count grows (beyond 2-3 test seats for LLMs), we observe decreasing order accuracy, fewer successful attacks, and minimal supply-center gains. Ultimately, LLMs lose traction in highly interactive scenarios, underscoring how partial observability and shifting alliances further intensify the multi-agent complexity. </p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/hanabi-human-performance.png" width="90%">
                  <p>We collected 54,977 human-played Hanabi games from BoardGameGeek, spanning 2- to 5-player settings. This figure plots the human score distribution, highlighting quartiles (Q1--Q4) around a typical range of 15--25 points. While some LLMs do show patterns of declining performance with more agents, none approach even the first quartile of human scores. This underscores the <strong>significant gap</strong> in cooperative planning under hidden-information constraints—despite Hanabi's narrower branching factor relative to some competitive games. </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>
  <!--/ Results -->

  
  <!-- Share Buttons -->
  <section class="section" id="Share">

  <div class="column has-text-centered">
    <h2 class="title is-3 has-text-centered">Share <span style="color: var(--color-octo-red);"><b>SPIN</b></span>-<span
      style="color: var(--color-tools-blue);"><b>Bench</b></span></h2>
    <!-- Social Media Share Buttons -->
    <div class="social-share-buttons" style="margin-top: 20px;">
      <!-- Twitter Share Button -->
      <a href="https://twitter.com/intent/tweet?text=Introducing+SPIN-Bench%3A+How+Well+Do+Large+Language+Models+Reason+Strategically+and+Socially%3F" 
         class="button is-info is-outlined"
         target="_blank">
        <span class="icon">
          <i class="fab fa-twitter"></i>
        </span>
        <span>Share on X (Twitter)</span>
      </a>
      
      <!-- LinkedIn Share Button -->
      <a href="https://www.linkedin.com/feed/?shareActive=true&shareUrl=https%3A%2F%2Fspinbench.github.io" 
         class="button is-info is-outlined"
         style="margin-left: 10px;"
         target="_blank">
        <span class="icon">
          <i class="fab fa-linkedin"></i>
        </span>
        <span>Share on LinkedIn</span>
      </a>
    </div>
  </div>
  </section>


  <!-- BibTeX -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">BibTeX</h2>
      <pre><code>@misc{yao2025spinbenchllmsplanstrategically,
        title={SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?}, 
        author={Jianzhu Yao and Kevin Wang and Ryan Hsieh and Haisu Zhou and Tianqing Zou and Zerui Cheng and Zhangyang Wang and Pramod Viswanath},
        year={2025},
        eprint={2503.12349},
        archivePrefix={arXiv},
        primaryClass={cs.AI},
        url={https://arxiv.org/abs/2503.12349}, 
  }
</code></pre>
    </div>
  </section>

  <!-- License -->
  <footer class="footer">
    <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a>, <a href="https://octotools.github.io/">OctoTools</a> and <a
              href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
                             
          <p>
            If you found any errors or typos in the website / trajectory viewer, please contact <a href="https://ece.princeton.edu/people/jianzhu-yao" target="_blank">Jianzhu Yao</a>
            <a href="mailto:jy0246@princeton.edu"><svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a> and <a href="https://www.kevin-ai.com/" target="_blank">Kevin Wang</a>
            <a href="mailto:kevinwang.1839@utexas.edu"><svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a>.</span>
          </p>
        </div>
      </div>
    </div>
    <!-- </div> -->
  </footer>

</body>
  <!-- Back to top button -->
  <button onclick="topFunction()" id="topButton" title="Go to top">
    <i class="fas fa-arrow-up"></i>
  </button>
</html>